{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai/clip-vit-large-patch14\n",
    "\n",
    "runwayml/stable-diffusion-v1-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface-cli download runwayml/stable-diffusion-v1-5 --local-dir ./models/stable-diffusion-v1-5\n",
    "\n",
    "huggingface-cli download openai/clip-vit-large-patch14 --local-dir ./models/clip-vit-large-patch14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py:56: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/public/congfeng4/AutoData/DriveDreamer/dreamer-models/dreamer_models/models/drivedreamer/convnext.py:179: UserWarning: Overwriting convnext_tiny in registry with dreamer_models.models.drivedreamer.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_tiny(pretrained=False, in_22k=False, **kwargs):\n",
      "/public/congfeng4/AutoData/DriveDreamer/dreamer-models/dreamer_models/models/drivedreamer/convnext.py:189: UserWarning: Overwriting convnext_small in registry with dreamer_models.models.drivedreamer.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_small(pretrained=False, in_22k=False, **kwargs):\n",
      "/public/congfeng4/AutoData/DriveDreamer/dreamer-models/dreamer_models/models/drivedreamer/convnext.py:199: UserWarning: Overwriting convnext_base in registry with dreamer_models.models.drivedreamer.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
      "/public/congfeng4/AutoData/DriveDreamer/dreamer-models/dreamer_models/models/drivedreamer/convnext.py:209: UserWarning: Overwriting convnext_large in registry with dreamer_models.models.drivedreamer.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
      "/public/congfeng4/AutoData/DriveDreamer/dreamer-models/dreamer_models/models/drivedreamer/convnext.py:219: UserWarning: Overwriting convnext_xlarge in registry with dreamer_models.models.drivedreamer.convnext.convnext_xlarge. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "{'mid_block_add_attention', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv', 'latents_mean', 'shift_factor', 'scaling_factor', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /public/congfeng4/AutoData/DriveDreamer/models/models--runwayml--stable-diffusion-v1-5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'variance_type', 'clip_sample_range', 'rescale_betas_zero_snr', 'thresholding', 'sample_max_value', 'timestep_spacing', 'dynamic_thresholding_ratio', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "{'use_linear_projection', 'dropout', 'conv_out_kernel', 'encoder_hid_dim_type', 'num_attention_heads', 'class_embed_type', 'resnet_skip_time_act', 'addition_embed_type_num_heads', 'addition_time_embed_dim', 'encoder_hid_dim', 'cross_attention_norm', 'mid_block_type', 'attention_type', 'mid_block_only_cross_attention', 'time_embedding_dim', 'timestep_post_act', 'reverse_transformer_layers_per_block', 'resnet_time_scale_shift', 'num_class_embeds', 'time_embedding_type', 'addition_embed_type', 'dual_cross_attention', 'conv_in_kernel', 'class_embeddings_concat', 'resnet_out_scale_factor', 'only_cross_attention', 'time_cond_proj_dim', 'time_embedding_act_fn', 'upcast_attention', 'projection_class_embeddings_input_dim', 'transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /public/congfeng4/AutoData/DriveDreamer/models/models--runwayml--stable-diffusion-v1-5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "2025-11-14 21:09:07,021 Config:\n",
      "{'dataloaders': {'test': {'batch_size_per_gpu': 1,\n",
      "                          'data_or_config': '/public/congfeng4/AutoData/DriveDreamer/data/v1.0-mini/cam_all_val/v0.0.1',\n",
      "                          'num_workers': 0,\n",
      "                          'transform': {'dd_name': 'image_hdmap',\n",
      "                                        'default_prompt': 'a realistic driving '\n",
      "                                                          'scene.',\n",
      "                                        'dst_size': 448,\n",
      "                                        'is_train': False,\n",
      "                                        'max_objs': 100,\n",
      "                                        'mode': 'long',\n",
      "                                        'pos_name': 'corner',\n",
      "                                        'prompt_name': 'sd',\n",
      "                                        'random_choice': False,\n",
      "                                        'type': 'DriveDreamerTransform'}},\n",
      "                 'train': {'batch_size_per_gpu': 4,\n",
      "                           'data_or_config': '/public/congfeng4/AutoData/DriveDreamer/data/v1.0-mini/cam_all_train/v0.0.1',\n",
      "                           'num_workers': 2,\n",
      "                           'sampler': {'type': 'DefaultSampler'},\n",
      "                           'transform': {'dd_name': 'image_hdmap',\n",
      "                                         'default_prompt': 'a realistic '\n",
      "                                                           'driving scene.',\n",
      "                                         'dst_size': 448,\n",
      "                                         'is_train': True,\n",
      "                                         'max_objs': 100,\n",
      "                                         'mode': 'long',\n",
      "                                         'pos_name': 'corner',\n",
      "                                         'prompt_name': 'sd',\n",
      "                                         'random_choice': True,\n",
      "                                         'type': 'DriveDreamerTransform'}}},\n",
      " 'launch': {'deepspeed_config': None,\n",
      "            'distributed_type': None,\n",
      "            'gpu_ids': [0],\n",
      "            'num_machines': 1},\n",
      " 'models': {'drivedreamer': {'add_in_channels': 8,\n",
      "                             'grounding_downsampler_cfg': {'in_dim': 3,\n",
      "                                                           'mid_dim': 4,\n",
      "                                                           'out_dim': 8,\n",
      "                                                           'type': 'GroundingDownSampler'},\n",
      "                             'noise_scheduler_type': 'DDPMScheduler',\n",
      "                             'position_net_cfg': {'box_dim': 16,\n",
      "                                                  'feature_type': 'text_only',\n",
      "                                                  'in_dim': 768,\n",
      "                                                  'mid_dim': 512,\n",
      "                                                  'type': 'PositionNet'},\n",
      "                             'unet_type': 'UNet2DConditionModel'},\n",
      "            'pipeline_name': 'StableDiffusionControlPipeline',\n",
      "            'pretrained': 'runwayml/stable-diffusion-v1-5',\n",
      "            'weight_path': None,\n",
      "            'with_ema': True},\n",
      " 'optimizers': {'lr': 5e-05, 'type': 'AdamW', 'weight_decay': 0.0},\n",
      " 'project_dir': '/public/congfeng4/AutoData/DriveDreamer/exp/drivedreamer-img_mini',\n",
      " 'schedulers': {'name': 'constant', 'num_warmup_steps': 100},\n",
      " 'test': {'guidance_scale': 7.5,\n",
      "          'mixed_precision': 'fp16',\n",
      "          'save_dir': '/public/congfeng4/AutoData/DriveDreamer/exp/drivedreamer-img_mini/vis'},\n",
      " 'train': {'activation_checkpointing': False,\n",
      "           'checkpoint_interval': 1,\n",
      "           'checkpoint_total_limit': 10,\n",
      "           'gradient_accumulation_steps': 1,\n",
      "           'log_interval': 100,\n",
      "           'log_with': 'tensorboard',\n",
      "           'max_epochs': 20,\n",
      "           'mixed_precision': 'fp16',\n",
      "           'resume': False,\n",
      "           'with_ema': True}}\n",
      "2025-11-14 21:09:07,025 ModuleDict(\n",
      "  (drivedreamer): DriveDreamerModel(\n",
      "    (conv_in): Conv2d(12, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (grounding_downsampler): GroundingDownSampler(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (3): SiLU()\n",
      "        (4): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (position_net): PositionNet(\n",
      "      (linears): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (3): SiLU()\n",
      "        (4): Linear(in_features=512, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (fusers): ModuleList(\n",
      "      (0-1): 2 x GatedSelfAttentionDense(\n",
      "        (linear): Linear(in_features=768, out_features=320, bias=True)\n",
      "        (attn): Attention(\n",
      "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "          (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "          (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (net): ModuleList(\n",
      "            (0): GEGLU(\n",
      "              (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "            )\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2-3): 2 x GatedSelfAttentionDense(\n",
      "        (linear): Linear(in_features=768, out_features=640, bias=True)\n",
      "        (attn): Attention(\n",
      "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "          (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "          (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (net): ModuleList(\n",
      "            (0): GEGLU(\n",
      "              (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "            )\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4-8): 5 x GatedSelfAttentionDense(\n",
      "        (linear): Linear(in_features=768, out_features=1280, bias=True)\n",
      "        (attn): Attention(\n",
      "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (net): ModuleList(\n",
      "            (0): GEGLU(\n",
      "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "            )\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9-11): 3 x GatedSelfAttentionDense(\n",
      "        (linear): Linear(in_features=768, out_features=640, bias=True)\n",
      "        (attn): Attention(\n",
      "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "          (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "          (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (net): ModuleList(\n",
      "            (0): GEGLU(\n",
      "              (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "            )\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (12-14): 3 x GatedSelfAttentionDense(\n",
      "        (linear): Linear(in_features=768, out_features=320, bias=True)\n",
      "        (attn): Attention(\n",
      "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "          (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "          (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (net): ModuleList(\n",
      "            (0): GEGLU(\n",
      "              (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "            )\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (15): GatedSelfAttentionDense(\n",
      "        (linear): Linear(in_features=768, out_features=1280, bias=True)\n",
      "        (attn): Attention(\n",
      "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (net): ModuleList(\n",
      "            (0): GEGLU(\n",
      "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "            )\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2025-11-14 21:09:07,028 num_processes: 1, process_index: 0, data_size: 11188, batch_size: 4, epoch_size: 2797\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/./dreamer-train/projects/launch.py\", line 40, in <module>\n",
      "    main()\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/./dreamer-train/projects/launch.py\", line 36, in main\n",
      "    launch_from_config(config_path, ','.join(opts.runners))\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/dreamer-train/dreamer_train/distributed/launch.py\", line 169, in launch_from_config\n",
      "    run_tasks(config, runners)\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/dreamer-train/dreamer_train/distributed/run_task.py\", line 27, in run_tasks\n",
      "    runner.train()\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/dreamer-train/dreamer_train/trainers/trainer.py\", line 514, in train\n",
      "    batch_dict = next(dataloader_iter)\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/accelerate/data_loader.py\", line 567, in __iter__\n",
      "    current_batch = next(dataloader_iter)\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_utils.py\", line 705, in reraise\n",
      "    raise exception\n",
      "KeyError: Caught KeyError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/public/congfeng4/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/dreamer-datasets/dreamer_datasets/datasets/base_dataset.py\", line 100, in __getitem__\n",
      "    data_dict = self.transform(data_dict)\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/dreamer-train/projects/DriveDreamer/drivedreamer/transforms.py\", line 67, in __call__\n",
      "    data_dict = self.pos_transform(data_dict)\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/dreamer-train/projects/DriveDreamer/drivedreamer/transforms.py\", line 286, in __call__\n",
      "    keeps = self.choose_objs(data_dict)\n",
      "  File \"/public/congfeng4/AutoData/DriveDreamer/dreamer-train/projects/DriveDreamer/drivedreamer/transforms.py\", line 211, in choose_objs\n",
      "    corners = data_dict['corners']\n",
      "KeyError: 'corners'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./dreamer-train/projects/launch.py \\\n",
    "        --project_name DriveDreamer \\\n",
    "        --config_name drivedreamer-img_mini \\\n",
    "        --runners drivedreamer.DriveDreamerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git reset HEAD <file>...\" to unstage)\n",
      "\n",
      "\t\u001b[32mmodified:   .gitignore\u001b[m\n",
      "\t\u001b[32mmodified:   ENV.py\u001b[m\n",
      "\t\u001b[32mmodified:   train.ipynb\u001b[m\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
      "\n",
      "\t\u001b[31mmodified:   train.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\n",
      "\t\u001b[31mdownload-models.sh\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
